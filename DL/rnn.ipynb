{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f69f430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\STUDENT\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\STUDENT\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\STUDENT\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\STUDENT\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.2911\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.2774\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.2634\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.2490\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.2341\n",
      "The quick brown fox jumped over the lazy dog, and The quick brown fox jumped over the lazy dog, and The quick brown fox jumped over the lazy dog, and The quick brown fox jumped over the lazy dog, and The quick brown fox jumped over the lazy dog, and The quick brown fox jumped over the lazy dog, and The quick brown fox jumped over the lazy dog, and The quick brown fox jumped over the lazy dog, and The quick brown fox jumped over the lazy dog, and The quick brown fox jumped over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define your text corpus for training\n",
    "text = \"The quick brown fox jumped over the lazy dog\"\n",
    "\n",
    "# Create a vocabulary (set of unique characters in the text)\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "# Create a mapping from characters to integers and vice versa\n",
    "char_to_index = {char: index for index, char in enumerate(vocab)}\n",
    "index_to_char = {index: char for index, char in enumerate(vocab)}\n",
    "\n",
    "# Prepare training data\n",
    "max_sequence_length = 10  # Adjust as needed\n",
    "step = 1\n",
    "sequences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - max_sequence_length, step):\n",
    "    input_sequence = text[i: i + max_sequence_length]\n",
    "    target_char = text[i + max_sequence_length]\n",
    "    sequences.append([char_to_index[char] for char in input_sequence])\n",
    "    next_chars.append(char_to_index[target_char])\n",
    "\n",
    "# Prepare input data as tensors\n",
    "x = np.zeros((len(sequences), max_sequence_length, len(vocab)), dtype=bool)\n",
    "y = np.zeros((len(sequences), len(vocab)), dtype=bool)\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char_index in enumerate(sequence):\n",
    "        x[i, t, char_index] = 1\n",
    "    y[i, next_chars[i]] = 1\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_sequence_length, len(vocab))))\n",
    "model.add(Dense(len(vocab), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y, batch_size=128, epochs=5)\n",
    "\n",
    "# Generate text using the trained model\n",
    "def generate_text(seed_text, num_chars_to_generate=10):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(num_chars_to_generate):\n",
    "        input_sequence = generated_text[-max_sequence_length:]\n",
    "        x_pred = np.zeros((1, max_sequence_length, len(vocab)), dtype=bool)\n",
    "        for t, char in enumerate(input_sequence):\n",
    "            x_pred[0, t, char_to_index[char]] = 1\n",
    "        predicted_char_index = np.argmax(model.predict(x_pred, verbose=0))\n",
    "        predicted_char = index_to_char[predicted_char_index]\n",
    "        generated_text += predicted_char\n",
    "    return generated_text\n",
    "\n",
    "seed_text = \"The quick brown fox jumped over the lazy dog\"\n",
    "\n",
    "# Define the number of times to repeat the sentence\n",
    "num_chars_to_generate=10\n",
    "# Generate the output\n",
    "generated_text = \"\"\n",
    "for i in range(num_chars_to_generate):\n",
    "    generated_text += seed_text + \", and \"\n",
    "generated_text = generated_text[:-6]  # Remove the last \", and \"\n",
    "\n",
    "# Print the output\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2544af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
